{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/rXi/HpDA2NS/rqjDdVlk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zamoraricardo15/Intro-to-Health-Informatics/blob/main/Covid_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approximation for misinformation estimation: COVID-19"
      ],
      "metadata": {
        "id": "9KF8c68roZih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S0PFHI7zQNR",
        "outputId": "541a47bc-0d46-4926-e253-062b61af4823"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2VhgwtwYmAz",
        "outputId": "8f530587-b66f-4147-9daa-a42d79b55cd3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Collecting tensorflow<2.12,>=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 588.3 MB 14 kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.19.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (4.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.1.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.28.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.21.6)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.50.0)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-22.11.23-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (57.4.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 72.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.14.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-22.11.23 keras-2.11.0 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-text-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iPNwmPtXPXOo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "import zipfile\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.utils import get_file\n",
        "from sklearn.model_selection import train_test_split\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df = pd.DataFrame(dict(title=[], isFakeNews=[], src=[]))"
      ],
      "metadata": {
        "id": "ySgQZnBNRxP5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fake and real news dataset:\n",
        "https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
      ],
      "metadata": {
        "id": "1wov_d3zo6Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fakecsv_df = pd.read_csv('/content/Fake.csv')\n",
        "truecsv_df = pd.read_csv('/content/True.csv')\n",
        "\n",
        "fakecsv_df['isFakeNews'] = True\n",
        "truecsv_df['isFakeNews'] = False\n",
        "fakecsv_df['src'] = 'Fake-and-real-news-dataset'\n",
        "truecsv_df['src'] = 'Fake-and-real-news-dataset'\n",
        "fake_df = fake_df\\\n",
        "                    .append(fakecsv_df[['title', 'isFakeNews', 'src']])\\\n",
        "                    .append(truecsv_df[['title', 'isFakeNews', 'src']])"
      ],
      "metadata": {
        "id": "UOYIfb03TB38"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FakeNewsNet:\n",
        "https://github.com/KaiDMML/FakeNewsNet"
      ],
      "metadata": {
        "id": "WaUnzXIYpWvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "politic_df = pd.read_csv(\"/content/politifact_real.csv\")\n",
        "politifake_df = pd.read_csv(\"/content/politifact_fake.csv\")\n",
        "politic_df['isFakeNews'] = False\n",
        "politifake_df['isFakeNews'] = True\n",
        "politifact_df = politic_df.append(politifake_df)\n",
        "politifact_df['src'] = 'FakeNewsNet/politifact'\n",
        "fake_df = fake_df.append(politifact_df[['title', 'isFakeNews', 'src']])"
      ],
      "metadata": {
        "id": "J3accQ3XSESU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "realgossip_df = pd.read_csv(\"/content/gossipcop_real.csv\")\n",
        "fakegossip_df = pd.read_csv(\"/content/gossipcop_fake.csv\")\n",
        "realgossip_df['isFakeNews'] = False\n",
        "fakegossip_df['isFakeNews'] = True\n",
        "gossipcop_df = realgossip_df.append(fakegossip_df)\n",
        "gossipcop_df['src'] = 'FakeNewsNet/gossipcop'\n",
        "fake_df = fake_df.append(gossipcop_df[['title', 'isFakeNews', 'src']])"
      ],
      "metadata": {
        "id": "AFz-HitDSRhi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COVID-19 Rumor Dataset:\n",
        "https://github.com/MickeysClubhouse/COVID-19-rumor-dataset"
      ],
      "metadata": {
        "id": "F_B5ZtrEqFZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newss_df = pd.read_csv('/content/en_dup.csv')\n",
        "newss_df = newss_df[~(newss_df['label'] == 'U')] \n",
        "newss_df['label'] = newss_df['label']  == 'F' \n",
        "newss_df['src'] = 'COVID-19-rumor-dataset' \n",
        "newss_df = newss_df[['label', 'content', 'src']].rename(columns={'label':'isFakeNews','content':'title'}) \n",
        "fake_df = fake_df.append(newss_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEugWKFVR2AG",
        "outputId": "e735e29c-3c6a-4fa1-9fde-105b91871c96"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3444cfac8d60>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newss_df['label'] = newss_df['label']  == 'F'\n",
            "<ipython-input-9-3444cfac8d60>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newss_df['src'] = 'COVID-19-rumor-dataset'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FakeCovid:\n",
        "https://github.com/Gautamshahi/FakeCovid"
      ],
      "metadata": {
        "id": "QKCDlARFqKRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jun_df = pd.read_csv(\"/content/FakeCovid_June2020.csv\")\n",
        "jul_df = pd.read_csv(\"/content/FakeCovid_July2020.csv\")\n",
        "jun_df.loc[jun_df['class'] == 'TRUE', 'isFakeNews'] = False\n",
        "jul_df.loc[jul_df['class'] == 'TRUE', 'isFakeNews'] = False\n",
        "jun_df['isFakeNews'].fillna(True, inplace = True)\n",
        "jul_df['isFakeNews'].fillna(True, inplace = True)\n",
        "jun_df['src'] = 'FakeCovid'\n",
        "jul_df['src'] = 'FakeCovid'\n",
        "fake_df = fake_df\\\n",
        "                    .append(jun_df[['ref_title', 'isFakeNews','src']].rename(columns={'ref_title':'title'}))\\\n",
        "                    .append(jul_df[['source_title', 'isFakeNews','src']].rename(columns={'source_title':'title'}))"
      ],
      "metadata": {
        "id": "q5HREYqIUHv7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df.dropna(inplace=True)\n",
        "fake_df.to_csv(\"data_fake_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "s0pI4ZzAVJQb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6hIDDKLbVfHs",
        "outputId": "1698a26b-7b69-4ff5-82e6-5011af0e58a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  isFakeNews  \\\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...         1.0   \n",
              "1   Drunk Bragging Trump Staffer Started Russian ...         1.0   \n",
              "2   Sheriff David Clarke Becomes An Internet Joke...         1.0   \n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...         1.0   \n",
              "4   Pope Francis Just Called Out Donald Trump Dur...         1.0   \n",
              "\n",
              "                          src  \n",
              "0  Fake-and-real-news-dataset  \n",
              "1  Fake-and-real-news-dataset  \n",
              "2  Fake-and-real-news-dataset  \n",
              "3  Fake-and-real-news-dataset  \n",
              "4  Fake-and-real-news-dataset  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27363ec9-7169-4ff8-8bc6-42009dee6277\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>isFakeNews</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27363ec9-7169-4ff8-8bc6-42009dee6277')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27363ec9-7169-4ff8-8bc6-42009dee6277 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27363ec9-7169-4ff8-8bc6-42009dee6277');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = \"/content/data_fake_df.csv\"\n",
        "df = pd.read_csv(df_data)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "K0MT9WGew7V_",
        "outputId": "34e3ec89-313f-4826-91aa-eb9e4ea86916"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  isFakeNews  \\\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...         1.0   \n",
              "1   Drunk Bragging Trump Staffer Started Russian ...         1.0   \n",
              "2   Sheriff David Clarke Becomes An Internet Joke...         1.0   \n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...         1.0   \n",
              "4   Pope Francis Just Called Out Donald Trump Dur...         1.0   \n",
              "\n",
              "                          src  \n",
              "0  Fake-and-real-news-dataset  \n",
              "1  Fake-and-real-news-dataset  \n",
              "2  Fake-and-real-news-dataset  \n",
              "3  Fake-and-real-news-dataset  \n",
              "4  Fake-and-real-news-dataset  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b67e6df-cb36-4b06-84fc-480b748e8f6a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>isFakeNews</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b67e6df-cb36-4b06-84fc-480b748e8f6a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b67e6df-cb36-4b06-84fc-480b748e8f6a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b67e6df-cb36-4b06-84fc-480b748e8f6a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lenmmax = df[\"title\"].apply(lambda x : len(x.split())).max()\n",
        "worcount = 100000\n",
        "length_data = len(df)\n",
        "length_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH75tGgAxK3h",
        "outputId": "7db923dd-7882-4352-c985-a5d968e1e71d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86002"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(dataset, length_data, val_split=0.2, shuffle=True, shuffle_size=50000):\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(shuffle_size, seed=42)\n",
        "    train_size = int((1-val_split) * length_data)\n",
        "    val_size = int(val_split * length_data)\n",
        "    try:\n",
        "      train_ds = dataset.take(train_size).map(lambda x : (x[\"title\"], x[\"isFakeNews\"]))\n",
        "      val_ds = dataset.skip(train_size).take(val_size).map(lambda x : (x[\"title\"], x[\"isFakeNews\"]))\n",
        "    except:\n",
        "      train_ds = dataset.take(train_size)\n",
        "      val_ds = dataset.skip(train_size).take(val_size)\n",
        "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return train_ds, val_ds"
      ],
      "metadata": {
        "id": "Oevo73bBZUZO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.data.experimental.make_csv_dataset(df_data, select_columns=[\n",
        "    \"title\",\n",
        "    \"isFakeNews\"                                                                    \n",
        "], batch_size=BATCH_SIZE)\n",
        "\n",
        "train_ds, val_ds = train_test_split(ds, length_data)\n",
        "val_ds, test_ds = train_test_split(val_ds, int(length_data * 0.2), val_split=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-rMZfsWZW70",
        "outputId": "25cb9d7b-d1de-4553-a576-a8699ddeec73"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_letters(text):\n",
        "  marksp = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "  sowords = {'whom', 'all', 'shouldn', 'wouldn', 'how', 's', 'they', 'were', 'mustn', 'after', 'who', 'its', 'our', 't', 'a', 'very', 'an', 'do', 'be', 'to', 'can', 'had', 'i', 'these', 'himself', 'up', 'just', 'them', 'now', 'has', 'too', 'below', 'did', 'shan', 'until', 'during', 'him', 'into', 'have', \"you'd\", 'haven', 'theirs', 'ourselves', 'once', \"isn't\", 'than', \"it's\", 'wasn', 'yours', \"mightn't\", 'here', 'ours', 'her', 'doing', 'd', 'yourself', 'y', 'before', 'does', 'then', 'between', 'some', 'with', \"needn't\", 'but', 'didn', \"shouldn't\", 'that', \"weren't\", 'which', 'or', \"hasn't\", 'own', 'about', 'what', \"aren't\", 'couldn', 'doesn', 'as', \"wouldn't\", 'hasn', 'no', 'm', 'hers', 'hadn', 'aren', 'while', 'will', \"don't\", \"shan't\", 'why', 'at', 'mightn', 'themselves', 'weren', \"that'll\", 'isn', 'only', 'the', 'been', \"couldn't\", 'don', 'should', 'same', 'both', 'where', 'was', 'me', 'through', \"hadn't\", 've', 'against', 'if', 'under', 'such', 'is', 'll', \"haven't\", 'ain', 're', \"didn't\", 'nor', 'not', 'being', 'are', 'your', 'over', 'off', 'having', 'by', \"won't\", 'myself', 'out', 'more', \"wasn't\", \"doesn't\", 'won', 'this', 'my', 'again', 'ma', 'his', 'when', 'you', 'there', 'herself', 'yourselves', 'itself', 'of', \"she's\", 'needn', 'we', \"mustn't\", 'above', \"you're\", 'so', 'it', \"should've\", 'am', 'he', 'those', 'further', 'she', 'down', 'on', \"you'll\", 'for', 'other', 'any', 'their', 'from', 'each', 'most', 'because', 'and', 'few', 'in', \"you've\", 'o'}\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.regex_replace(text, \"<[^>]+>\", \"\")\n",
        "  text = tf.strings.regex_replace(text, '[%s]' % marksp, \"\")\n",
        "  for soword in sowords:\n",
        "    text = tf.strings.regex_replace(text, r\"\\b%s\\b\" % soword, \"\")\n",
        "  return text\n",
        "example1 = \"Is COVID really happening and is it deadly?\"\n",
        "print(check_letters(example1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gahK9NRuZYVP",
        "outputId": "cd38f517-d2fe-4305-99d7-ebdc938d959e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b' covid really happening    deadly', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def token_define(train_ds, worcount, lenmmax, output_mode = \"int\", standardize = \"lower_and_strip_marksp\"):\n",
        "  train_text = train_ds.map(lambda x, y : x)\n",
        "  ltokenl = tfl.TextVectorization(\n",
        "      standardize=standardize,\n",
        "      max_tokens=worcount,\n",
        "      output_sequence_length=lenmmax,\n",
        "      output_mode=output_mode\n",
        "  )\n",
        "  ltokenl.adapt(train_text)\n",
        "  return ltokenl"
      ],
      "metadata": {
        "id": "2qGMLwHIZdQX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_definition(train_ds, worcount, lenmmax=None, output_mode = \"int\", standardize = \"lower_and_strip_marksp\"):\n",
        "  train_text = train_ds.map(lambda x, y : x)\n",
        "  ltokenl = tfl.TextVectorization(\n",
        "      standardize=standardize,\n",
        "      max_tokens=worcount,\n",
        "      output_mode=output_mode\n",
        "  )\n",
        "  ltokenl.adapt(train_text)\n",
        "  return ltokenl"
      ],
      "metadata": {
        "id": "bAqbNotTauzd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ltokenl = token_definition(train_ds, worcount, lenmmax, standardize=check_letters)"
      ],
      "metadata": {
        "id": "plFWVQLpa0vD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_pretrained_initial(url, output_file, embedding_file, embedding_dim, vocabulary, worcount, lenmmax):\n",
        "  embedding_vecs = dict()\n",
        "  word_idx = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "  file_dir = get_file(output_file, url)\n",
        "  with zipfile.ZipFile(file_dir, \"r\") as f:\n",
        "    f.extractall(\"/content/\")\n",
        "  with open(embedding_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      embedding_vec = np.asarray(values[1:], dtype='float32')\n",
        "      embedding_vecs[word] = embedding_vec\n",
        "  embedding_matrix = np.zeros((worcount, embedding_dim))\n",
        "  for word, idx in word_idx.items():\n",
        "    if idx < worcount:\n",
        "      embedding_vec = embedding_vecs.get(word)\n",
        "      if embedding_vec is not None:\n",
        "        embedding_matrix[idx] = embedding_vec\n",
        "  \n",
        "  embedding = tfl.Embedding(worcount, embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=True, input_length=lenmmax, trainable=False)\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "iV7XMeBegXkq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_pretrained_after(url, output_file, embedding_file, embedding_dim, vocabulary, worcount, lenmmax=None):\n",
        "  embedding_vecs = dict()\n",
        "  word_idx = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "  file_dir = get_file(output_file, url)\n",
        "\n",
        "  with zipfile.ZipFile(file_dir, \"r\") as f:\n",
        "    f.extractall(\"/content/\")\n",
        "\n",
        "  with open(embedding_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      embedding_vec = np.asarray(values[1:], dtype='float32')\n",
        "      embedding_vecs[word] = embedding_vec\n",
        "\n",
        "  embedding_matrix = np.zeros((worcount, embedding_dim))\n",
        "  \n",
        "  for word, idx in word_idx.items():\n",
        "    if idx < worcount:\n",
        "      embedding_vec = embedding_vecs.get(word)\n",
        "      if embedding_vec is not None:\n",
        "        embedding_matrix[idx] = embedding_vec\n",
        "  \n",
        "  embedding = tfl.Embedding(worcount, embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), input_dim=len(vocabulary), trainable=False)\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "hEvynXMNgfPJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = ltokenl.get_vocabulary()\n",
        "embedding = check_pretrained_initial(\"https://nlp.stanford.edu/data/glove.6B.zip\", \"glove.6B.zip\", \"glove.6B.100d.txt\", 100, vocabulary=vocabulary, worcount=worcount, lenmmax=lenmmax)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LThLcntaghfx",
        "outputId": "35b8c533-60d6-4967-d47a-db832b5695d1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://nlp.stanford.edu/data/glove.6B.zip\n",
            "862182613/862182613 [==============================] - 160s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def review_fake_check(ltokenl, embedding_layer, worcount, lenmmax, optimizer='adam'):\n",
        "  model = Sequential(\n",
        "      [\n",
        "      ltokenl,\n",
        "      embedding_layer,\n",
        "      tfl.Bidirectional(tfl.LSTM(128, return_sequences=True, input_shape=(worcount, lenmmax))),\n",
        "      tfl.Bidirectional(tfl.LSTM(128, return_sequences=False)),\n",
        "      #tfl.Bidirectional(tfl.LSTM(128)),\n",
        "      tfl.Dropout(0.2),\n",
        "      #tfl.Dense(128, activation='relu')\n",
        "      #tfl.Dense(64, activation='relu')\n",
        "      tfl.Dense(1, activation='sigmoid')\n",
        "      ]\n",
        "  )\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['acc'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "_KEsT_NNgnhc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = review_fake_check(ltokenl, embedding, worcount, lenmmax)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjZajTangp4r",
        "outputId": "bcea3482-b509-42c8-e2ae-8815f5651ca8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         10000000  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 256)        234496    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 256)              394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,628,993\n",
            "Trainable params: 628,993\n",
            "Non-trainable params: 10,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, TerminateOnNaN, EarlyStopping\n",
        "checkpoint_path = \"/content/\"\n",
        "callbacks = [\n",
        "             TensorBoard(),\n",
        "             ModelCheckpoint(checkpoint_path),\n",
        "             ReduceLROnPlateau(),\n",
        "             TerminateOnNaN(),\n",
        "             EarlyStopping(patience=2)\n",
        "]"
      ],
      "metadata": {
        "id": "ros6Geq0kd4P"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_ds, validation_ds = None, val_split = 0.2, batch_size = BATCH_SIZE, epochs=5, callbacks=callbacks):\n",
        "  if validation_ds is None:\n",
        "    trainable_check = model.fit(training_ds, validation_split=val_split, batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
        "  else:\n",
        "    trainable_check = model.fit(training_ds, validation_data=validation_ds, batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
        "  return trainable_check"
      ],
      "metadata": {
        "id": "rbWgzZKukvFS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_check = train_model(model, train_ds, val_ds, epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQhIjxP6kx9z",
        "outputId": "20cbda6f-e802-4601-8668-6d22f8323d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "  68801/Unknown - 20002s 290ms/step - loss: 0.0416 - acc: 0.9836"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 9). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r68801/68801 [==============================] - 21008s 305ms/step - loss: 0.0416 - acc: 0.9836 - val_loss: 0.0088 - val_acc: 0.9966 - lr: 0.0010\n",
            "Epoch 2/3\n",
            " 6342/68801 [=>............................] - ETA: 4:55:56 - loss: 0.0101 - acc: 0.9960"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflowjs as tfjs\n",
        "def model_to_tfhs(model, output_dir):\n",
        "  tfjs.converters.save_keras_model(model, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "7xiFIQmymRm4",
        "outputId": "75adf493-133b-44b3-a598-1deee3a7fc35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a6c50e038884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflowjs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfjs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_to_tfhs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtfjs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflowjs'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_result = model().predict(embedding)\n",
        "\n",
        "prediction = []\n",
        "for i in range (len(predicted_result)):\n",
        "  if predicted_result[i].item() > 0.5:\n",
        "    prediction.append(1)\n",
        "  else:\n",
        "    prediction.append(0)\n",
        "    \n",
        "\n",
        "#accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(list(test_ds), prediction)\n",
        "\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "TQ_VCJKAR3Qh",
        "outputId": "76c2d8c7-5e19-4c43-92b4-dc47f7c8a401"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f6898c2f9f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpredicted_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/model2\")"
      ],
      "metadata": {
        "id": "GvLOEqOumSVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "OhnfaZZfmZcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "clone = load_model('/content/model1')"
      ],
      "metadata": {
        "id": "MBUUnAiRmajR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv saved_model '/content/backup'"
      ],
      "metadata": {
        "id": "_YEfbqeqmkpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}