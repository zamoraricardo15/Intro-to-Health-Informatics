{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPNoQyyiEH57UgWeZqIckc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zamoraricardo15/Intro-to-Health-Informatics/blob/main/Covid_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approximation for misinformation estimation: COVID-19"
      ],
      "metadata": {
        "id": "9KF8c68roZih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4S0PFHI7zQNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2VhgwtwYmAz",
        "outputId": "774547cb-2ba6-4aae-ae21-994002539452"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.8/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.8/dist-packages)\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tensorflow<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.11.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (4.1.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.28.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.50.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (22.11.23)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.14.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.0.9)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.8/dist-packages)\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPNwmPtXPXOo",
        "outputId": "c46a9b84-0c5b-4327-fba7-a071bd8991e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "import zipfile\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.utils import get_file\n",
        "from sklearn.model_selection import train_test_split\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df = pd.DataFrame(dict(title=[], isFakeNews=[], src=[]))"
      ],
      "metadata": {
        "id": "ySgQZnBNRxP5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fake and real news dataset:\n",
        "https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
      ],
      "metadata": {
        "id": "1wov_d3zo6Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fakecsv_df = pd.read_csv('/content/Fake.csv')\n",
        "truecsv_df = pd.read_csv('/content/True.csv')\n",
        "\n",
        "fakecsv_df['isFakeNews'] = True\n",
        "truecsv_df['isFakeNews'] = False\n",
        "fakecsv_df['src'] = 'Fake-and-real-news-dataset'\n",
        "truecsv_df['src'] = 'Fake-and-real-news-dataset'\n",
        "fake_df = fake_df\\\n",
        "                    .append(fakecsv_df[['title', 'isFakeNews', 'src']])\\\n",
        "                    .append(truecsv_df[['title', 'isFakeNews', 'src']])"
      ],
      "metadata": {
        "id": "UOYIfb03TB38"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FakeNewsNet:\n",
        "https://github.com/KaiDMML/FakeNewsNet"
      ],
      "metadata": {
        "id": "WaUnzXIYpWvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "politic_df = pd.read_csv(\"/content/politifact_real.csv\")\n",
        "politifake_df = pd.read_csv(\"/content/politifact_fake.csv\")\n",
        "politic_df['isFakeNews'] = False\n",
        "politifake_df['isFakeNews'] = True\n",
        "politifact_df = politic_df.append(politifake_df)\n",
        "politifact_df['src'] = 'FakeNewsNet/politifact'\n",
        "fake_df = fake_df.append(politifact_df[['title', 'isFakeNews', 'src']])"
      ],
      "metadata": {
        "id": "J3accQ3XSESU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "realgossip_df = pd.read_csv(\"/content/gossipcop_real.csv\")\n",
        "fakegossip_df = pd.read_csv(\"/content/gossipcop_fake.csv\")\n",
        "realgossip_df['isFakeNews'] = False\n",
        "fakegossip_df['isFakeNews'] = True\n",
        "gossipcop_df = realgossip_df.append(fakegossip_df)\n",
        "gossipcop_df['src'] = 'FakeNewsNet/gossipcop'\n",
        "fake_df = fake_df.append(gossipcop_df[['title', 'isFakeNews', 'src']])"
      ],
      "metadata": {
        "id": "AFz-HitDSRhi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COVID-19 Rumor Dataset:\n",
        "https://github.com/MickeysClubhouse/COVID-19-rumor-dataset"
      ],
      "metadata": {
        "id": "F_B5ZtrEqFZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newss_df = pd.read_csv('/content/en_dup.csv')\n",
        "newss_df = newss_df[~(newss_df['label'] == 'U')] \n",
        "newss_df['label'] = newss_df['label']  == 'F' \n",
        "newss_df['src'] = 'COVID-19-rumor-dataset' \n",
        "newss_df = newss_df[['label', 'content', 'src']].rename(columns={'label':'isFakeNews','content':'title'}) \n",
        "fake_df = fake_df.append(newss_df)"
      ],
      "metadata": {
        "id": "dEugWKFVR2AG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FakeCovid:\n",
        "https://github.com/Gautamshahi/FakeCovid"
      ],
      "metadata": {
        "id": "QKCDlARFqKRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jun_df = pd.read_csv(\"/content/FakeCovid_June2020.csv\")\n",
        "jul_df = pd.read_csv(\"/content/FakeCovid_July2020.csv\")\n",
        "jun_df.loc[jun_df['class'] == 'TRUE', 'isFakeNews'] = False\n",
        "jul_df.loc[jul_df['class'] == 'TRUE', 'isFakeNews'] = False\n",
        "jun_df['isFakeNews'].fillna(True, inplace = True)\n",
        "jul_df['isFakeNews'].fillna(True, inplace = True)\n",
        "jun_df['src'] = 'FakeCovid'\n",
        "jul_df['src'] = 'FakeCovid'\n",
        "fake_df = fake_df\\\n",
        "                    .append(jun_df[['ref_title', 'isFakeNews','src']].rename(columns={'ref_title':'title'}))\\\n",
        "                    .append(jul_df[['source_title', 'isFakeNews','src']].rename(columns={'source_title':'title'}))"
      ],
      "metadata": {
        "id": "q5HREYqIUHv7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df.dropna(inplace=True)\n",
        "fake_df.to_csv(\"data_fake_df.csv\", index=False)"
      ],
      "metadata": {
        "id": "s0pI4ZzAVJQb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "6hIDDKLbVfHs",
        "outputId": "42fa4318-1a21-4884-845e-00270ec56279"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  isFakeNews  \\\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...         1.0   \n",
              "1   Drunk Bragging Trump Staffer Started Russian ...         1.0   \n",
              "2   Sheriff David Clarke Becomes An Internet Joke...         1.0   \n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...         1.0   \n",
              "4   Pope Francis Just Called Out Donald Trump Dur...         1.0   \n",
              "\n",
              "                          src  \n",
              "0  Fake-and-real-news-dataset  \n",
              "1  Fake-and-real-news-dataset  \n",
              "2  Fake-and-real-news-dataset  \n",
              "3  Fake-and-real-news-dataset  \n",
              "4  Fake-and-real-news-dataset  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-923ed3df-136d-4c1e-b365-76f6bb2100c9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>isFakeNews</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-923ed3df-136d-4c1e-b365-76f6bb2100c9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-923ed3df-136d-4c1e-b365-76f6bb2100c9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-923ed3df-136d-4c1e-b365-76f6bb2100c9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = \"/content/data_fake_df.csv\"\n",
        "df = pd.read_csv(df_data)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "K0MT9WGew7V_",
        "outputId": "a8de92f7-dc6f-4e81-f1db-d9dfc48929e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  isFakeNews  \\\n",
              "0   Donald Trump Sends Out Embarrassing New Year’...         1.0   \n",
              "1   Drunk Bragging Trump Staffer Started Russian ...         1.0   \n",
              "2   Sheriff David Clarke Becomes An Internet Joke...         1.0   \n",
              "3   Trump Is So Obsessed He Even Has Obama’s Name...         1.0   \n",
              "4   Pope Francis Just Called Out Donald Trump Dur...         1.0   \n",
              "\n",
              "                          src  \n",
              "0  Fake-and-real-news-dataset  \n",
              "1  Fake-and-real-news-dataset  \n",
              "2  Fake-and-real-news-dataset  \n",
              "3  Fake-and-real-news-dataset  \n",
              "4  Fake-and-real-news-dataset  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ced86f5a-cb87-4d7b-99ae-2e9099a706d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>isFakeNews</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Fake-and-real-news-dataset</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ced86f5a-cb87-4d7b-99ae-2e9099a706d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ced86f5a-cb87-4d7b-99ae-2e9099a706d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ced86f5a-cb87-4d7b-99ae-2e9099a706d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lenmmax = df[\"title\"].apply(lambda x : len(x.split())).max()\n",
        "worcount = 100000\n",
        "length_data = len(df)\n",
        "length_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH75tGgAxK3h",
        "outputId": "aaad47c7-f140-4ed4-d82c-2a1ba831b2a6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86002"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(dataset, length_data, val_split=0.2, shuffle=True, shuffle_size=50000):\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(shuffle_size, seed=42)\n",
        "    train_size = int((1-val_split) * length_data)\n",
        "    val_size = int(val_split * length_data)\n",
        "    try:\n",
        "      train_ds = dataset.take(train_size).map(lambda x : (x[\"title\"], x[\"isFakeNews\"]))\n",
        "      val_ds = dataset.skip(train_size).take(val_size).map(lambda x : (x[\"title\"], x[\"isFakeNews\"]))\n",
        "    except:\n",
        "      train_ds = dataset.take(train_size)\n",
        "      val_ds = dataset.skip(train_size).take(val_size)\n",
        "    train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return train_ds, val_ds"
      ],
      "metadata": {
        "id": "Oevo73bBZUZO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.data.experimental.make_csv_dataset(df_data, select_columns=[\n",
        "    \"title\",\n",
        "    \"isFakeNews\"                                                                    \n",
        "], batch_size=BATCH_SIZE)\n",
        "\n",
        "train_ds, val_ds = train_test_split(ds, length_data)\n",
        "val_ds, test_ds = train_test_split(val_ds, int(length_data * 0.2), val_split=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-rMZfsWZW70",
        "outputId": "358fef96-5ba5-4d05-f668-55fc720bbd53"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_letters(text):\n",
        "  marksp = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "  sowords = {'whom', 'all', 'shouldn', 'wouldn', 'how', 's', 'they', 'were', 'mustn', 'after', 'who', 'its', 'our', 't', 'a', 'very', 'an', 'do', 'be', 'to', 'can', 'had', 'i', 'these', 'himself', 'up', 'just', 'them', 'now', 'has', 'too', 'below', 'did', 'shan', 'until', 'during', 'him', 'into', 'have', \"you'd\", 'haven', 'theirs', 'ourselves', 'once', \"isn't\", 'than', \"it's\", 'wasn', 'yours', \"mightn't\", 'here', 'ours', 'her', 'doing', 'd', 'yourself', 'y', 'before', 'does', 'then', 'between', 'some', 'with', \"needn't\", 'but', 'didn', \"shouldn't\", 'that', \"weren't\", 'which', 'or', \"hasn't\", 'own', 'about', 'what', \"aren't\", 'couldn', 'doesn', 'as', \"wouldn't\", 'hasn', 'no', 'm', 'hers', 'hadn', 'aren', 'while', 'will', \"don't\", \"shan't\", 'why', 'at', 'mightn', 'themselves', 'weren', \"that'll\", 'isn', 'only', 'the', 'been', \"couldn't\", 'don', 'should', 'same', 'both', 'where', 'was', 'me', 'through', \"hadn't\", 've', 'against', 'if', 'under', 'such', 'is', 'll', \"haven't\", 'ain', 're', \"didn't\", 'nor', 'not', 'being', 'are', 'your', 'over', 'off', 'having', 'by', \"won't\", 'myself', 'out', 'more', \"wasn't\", \"doesn't\", 'won', 'this', 'my', 'again', 'ma', 'his', 'when', 'you', 'there', 'herself', 'yourselves', 'itself', 'of', \"she's\", 'needn', 'we', \"mustn't\", 'above', \"you're\", 'so', 'it', \"should've\", 'am', 'he', 'those', 'further', 'she', 'down', 'on', \"you'll\", 'for', 'other', 'any', 'their', 'from', 'each', 'most', 'because', 'and', 'few', 'in', \"you've\", 'o'}\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.regex_replace(text, \"<[^>]+>\", \"\")\n",
        "  text = tf.strings.regex_replace(text, '[%s]' % marksp, \"\")\n",
        "  for soword in sowords:\n",
        "    text = tf.strings.regex_replace(text, r\"\\b%s\\b\" % soword, \"\")\n",
        "  return text\n",
        "example1 = \"Is COVID really happening and is it deadly?\"\n",
        "print(check_letters(example1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gahK9NRuZYVP",
        "outputId": "b014a8c3-9e02-40ee-9d0e-6d2ef35f9cac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b' covid really happening    deadly', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def token_define(train_ds, worcount, lenmmax, output_mode = \"int\", standardize = \"lower_and_strip_marksp\"):\n",
        "  train_text = train_ds.map(lambda x, y : x)\n",
        "  ltokenl = tfl.TextVectorization(\n",
        "      standardize=standardize,\n",
        "      max_tokens=worcount,\n",
        "      output_sequence_length=lenmmax,\n",
        "      output_mode=output_mode\n",
        "  )\n",
        "  ltokenl.adapt(train_text)\n",
        "  return ltokenl"
      ],
      "metadata": {
        "id": "2qGMLwHIZdQX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token_definition(train_ds, worcount, lenmmax=None, output_mode = \"int\", standardize = \"lower_and_strip_marksp\"):\n",
        "  train_text = train_ds.map(lambda x, y : x)\n",
        "  ltokenl = tfl.TextVectorization(\n",
        "      standardize=standardize,\n",
        "      max_tokens=worcount,\n",
        "      output_mode=output_mode\n",
        "  )\n",
        "  ltokenl.adapt(train_text)\n",
        "  return ltokenl"
      ],
      "metadata": {
        "id": "bAqbNotTauzd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ltokenl = token_definition(train_ds, worcount, lenmmax, standardize=check_letters)"
      ],
      "metadata": {
        "id": "plFWVQLpa0vD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_pretrained_initial(url, output_file, embedding_file, embedding_dim, vocabulary, worcount, lenmmax):\n",
        "  embedding_vecs = dict()\n",
        "  word_idx = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "  file_dir = get_file(output_file, url)\n",
        "  with zipfile.ZipFile(file_dir, \"r\") as f:\n",
        "    f.extractall(\"/content/\")\n",
        "  with open(embedding_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      embedding_vec = np.asarray(values[1:], dtype='float32')\n",
        "      embedding_vecs[word] = embedding_vec\n",
        "  embedding_matrix = np.zeros((worcount, embedding_dim))\n",
        "  for word, idx in word_idx.items():\n",
        "    if idx < worcount:\n",
        "      embedding_vec = embedding_vecs.get(word)\n",
        "      if embedding_vec is not None:\n",
        "        embedding_matrix[idx] = embedding_vec\n",
        "  \n",
        "  embedding = tfl.Embedding(worcount, embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), mask_zero=True, input_length=lenmmax, trainable=False)\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "iV7XMeBegXkq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_pretrained_after(url, output_file, embedding_file, embedding_dim, vocabulary, worcount, lenmmax=None):\n",
        "  embedding_vecs = dict()\n",
        "  word_idx = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "  file_dir = get_file(output_file, url)\n",
        "\n",
        "  with zipfile.ZipFile(file_dir, \"r\") as f:\n",
        "    f.extractall(\"/content/\")\n",
        "\n",
        "  with open(embedding_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      embedding_vec = np.asarray(values[1:], dtype='float32')\n",
        "      embedding_vecs[word] = embedding_vec\n",
        "\n",
        "  embedding_matrix = np.zeros((worcount, embedding_dim))\n",
        "  \n",
        "  for word, idx in word_idx.items():\n",
        "    if idx < worcount:\n",
        "      embedding_vec = embedding_vecs.get(word)\n",
        "      if embedding_vec is not None:\n",
        "        embedding_matrix[idx] = embedding_vec\n",
        "  \n",
        "  embedding = tfl.Embedding(worcount, embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), input_dim=len(vocabulary), trainable=False)\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "hEvynXMNgfPJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = ltokenl.get_vocabulary()\n",
        "embedding = check_pretrained_initial(\"https://nlp.stanford.edu/data/glove.6B.zip\", \"glove.6B.zip\", \"glove.6B.100d.txt\", 100, vocabulary=vocabulary, worcount=worcount, lenmmax=lenmmax)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LThLcntaghfx",
        "outputId": "3fd48e8d-3893-4a8d-c9b6-3fc895470090"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://nlp.stanford.edu/data/glove.6B.zip\n",
            "862182613/862182613 [==============================] - 160s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def review_fake_check(ltokenl, embedding_layer, worcount, lenmmax, optimizer='adam'):\n",
        "  model = Sequential(\n",
        "      [\n",
        "      ltokenl,\n",
        "      embedding_layer,\n",
        "      tfl.Bidirectional(tfl.LSTM(128, return_sequences=True, input_shape=(worcount, lenmmax))),\n",
        "      tfl.Bidirectional(tfl.LSTM(128, return_sequences=False)),\n",
        "      tfl.Dropout(0.2),\n",
        "      tfl.Dense(128, activation='relu')\n",
        "      tfl.Dense(64, activation='relu')\n",
        "      tfl.Dense(1, activation='sigmoid')\n",
        "      ]\n",
        "  )\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics = ['acc'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "_KEsT_NNgnhc"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = review_fake_check(ltokenl, embedding, worcount, lenmmax)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjZajTangp4r",
        "outputId": "d822dbf5-38fc-44ef-c702-ab03110aabbc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         10000000  \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 256)        234496    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 256)              394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,628,993\n",
            "Trainable params: 628,993\n",
            "Non-trainable params: 10,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, TerminateOnNaN, EarlyStopping\n",
        "checkpoint_path = \"/content/\"\n",
        "callbacks = [\n",
        "             TensorBoard(),\n",
        "             ModelCheckpoint(checkpoint_path),\n",
        "             ReduceLROnPlateau(),\n",
        "             TerminateOnNaN(),\n",
        "             EarlyStopping(patience=2)\n",
        "]"
      ],
      "metadata": {
        "id": "ros6Geq0kd4P"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_ds, validation_ds = None, val_split = 0.2, batch_size = BATCH_SIZE, epochs=5, callbacks=callbacks):\n",
        "  if validation_ds is None:\n",
        "    trainable_check = model.fit(training_ds, validation_split=val_split, batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
        "  else:\n",
        "    trainable_check = model.fit(training_ds, validation_data=validation_ds, batch_size=batch_size, epochs=epochs, callbacks=callbacks)\n",
        "  return trainable_check"
      ],
      "metadata": {
        "id": "rbWgzZKukvFS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_check = train_model(model, train_ds, val_ds, epochs=3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQhIjxP6kx9z",
        "outputId": "607d78c6-39ef-497e-90c4-860624e07304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "   8927/Unknown - 2634s 295ms/step - loss: 0.0099 - accuracy: 0.9961"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflowjs as tfjs\n",
        "def model_to_tfhs(model, output_dir):\n",
        "  tfjs.converters.save_keras_model(model, output_dir)"
      ],
      "metadata": {
        "id": "7xiFIQmymRm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_result = model().predict(embedding)\n",
        "\n",
        "prediction = []\n",
        "for i in range (len(predicted_result)):\n",
        "  if predicted_result[i].item() > 0.5:\n",
        "    prediction.append(1)\n",
        "  else:\n",
        "    prediction.append(0)\n",
        "    \n",
        "\n",
        "#accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(list(test_ds), prediction)\n",
        "\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "TQ_VCJKAR3Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/model2\")"
      ],
      "metadata": {
        "id": "GvLOEqOumSVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "OhnfaZZfmZcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "clone = load_model('/content/model1')"
      ],
      "metadata": {
        "id": "MBUUnAiRmajR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv saved_model '/content/backup'"
      ],
      "metadata": {
        "id": "_YEfbqeqmkpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}